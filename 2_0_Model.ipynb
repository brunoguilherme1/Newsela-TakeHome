{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v0iucJB2L9qc"
   },
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vCiP_oTCJhPx"
   },
   "source": [
    "Given the 3-hour time constraint for this assignment, I prioritized developing a simple yet effective baseline model that could be iteratively improved with more time. Based on insights from the EDA notebook, where we observed a one-to-many relationship between topics and content, I opted for a retrieval and ranking approach.\n",
    "\n",
    "I used pretrained text embeddings to represent topics and content, then applied Euclidean distance to retrieve and rank the most relevant topics for each content item. For each candidate pair, I engineered additional features using a weighted combination of Word2Vec embeddings and TF-IDF scores to capture semantic overlap, and trained a LightGBM classifier to predict final relevance.\n",
    "\n",
    "\n",
    "This strategy provided a fast, scalable solution that balances simplicity and effectiveness within the limited time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-H04ZjvhJS1I"
   },
   "source": [
    "### Load and Install Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HXfunSVxSWRw",
    "outputId": "adb90d4f-f6dc-41db-c43a-9e00fff723eb"
   },
   "outputs": [],
   "source": [
    "!pip install gensim\n",
    "!pip install faiss-cpu\n",
    "!pip install stopwordsiso\n",
    "!pip install -q swifter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fU8feh3MJRr_",
    "outputId": "b8dc3294-7cd8-4f31-b520-cf1d425aacf7"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q0zXKvtYOpUk"
   },
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# ðŸ“ System & Utility\n",
    "# ===========================\n",
    "import os\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")  # Ignore all warnings\n",
    "from tqdm.notebook import tqdm  # Progress bar (Jupyter-friendly)\n",
    "\n",
    "# ===========================\n",
    "# ðŸ“Š Data & Math Libraries\n",
    "# ===========================\n",
    "import numpy as np\n",
    "from numpy.linalg import norm  # For vector norms\n",
    "import pandas as pd\n",
    "\n",
    "# ===========================\n",
    "# ðŸ“ˆ Visualization\n",
    "# ===========================\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud  # Generate word clouds\n",
    "\n",
    "# Matplotlib global settings\n",
    "matplotlib.rcParams['figure.figsize'] = (6, 4)\n",
    "plt.rc('font', weight='bold')\n",
    "\n",
    "# ===========================\n",
    "# ðŸ’¬ NLP & Embeddings\n",
    "# ===========================\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "import stopwordsiso as stopwords  # Multilingual stopwords\n",
    "from sentence_transformers import SentenceTransformer  # Sentence embeddings\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "# ===========================\n",
    "# ðŸ§  Text Vectorization & Similarity\n",
    "# ===========================\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import normalize  # Optional: normalize vectors\n",
    "\n",
    "\n",
    "# ===========================\n",
    "# ðŸ” Dimensionality Reduction\n",
    "# ===========================\n",
    "from sklearn.manifold import TSNE  # t-SNE for visualization of embeddings\n",
    "\n",
    "# ===========================\n",
    "# ðŸ§ª ML Evaluation & Modeling\n",
    "# ===========================\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    precision_score, recall_score, f1_score, fbeta_score, roc_auc_score\n",
    ")\n",
    "from imblearn.over_sampling import SMOTE  # Handle class imbalance\n",
    "from lightgbm import LGBMClassifier  # Gradient boosting model\n",
    "\n",
    "# ===========================\n",
    "# ðŸ’¾ Model & Index Handling\n",
    "# ===========================\n",
    "import joblib  # Save/load models\n",
    "import faiss   # Fast Approximate Nearest Neighbors (ANN) search\n",
    "import swifter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mS5JYBN1JkWa"
   },
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kPNqatg8Jjzj"
   },
   "outputs": [],
   "source": [
    "content = pd.read_csv(\"/content/drive/MyDrive/learning-equality-curriculum-recommendations (2)/content.csv\")\n",
    "topics  = pd.read_csv(\"/content/drive/MyDrive/learning-equality-curriculum-recommendations (2)/topics.csv\")\n",
    "correlations = pd.read_csv(\"/content/drive/MyDrive/learning-equality-curriculum-recommendations (2)/correlations.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sFZovCOsJ4bN"
   },
   "source": [
    "### Basic Clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wb_eTJ25NTiq"
   },
   "source": [
    "To prepare the data for NLP and retrieval using sentence-transformers, I performed basic cleaning: filling missing text fields, converting all columns to string, and combining relevant fields into unified text columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mrrawGYJ-Jsv"
   },
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# ðŸ“Œ Basic Cleanup\n",
    "# ----------------------------\n",
    "\n",
    "# Fill NA with empty strings where necessary\n",
    "for col in [\"title\", \"description\", \"channel\", \"category\", \"language\", \"parent\"]:\n",
    "    topics[col] = topics[col].fillna(\"\")\n",
    "\n",
    "for col in [\"title\", \"description\", \"text\", \"kind\", \"language\"]:\n",
    "    content[col] = content[col].fillna(\"\")\n",
    "\n",
    "# Ensure text columns are string type\n",
    "topics = topics.astype(str)\n",
    "content = content.astype(str)\n",
    "\n",
    "def build_text(row):\n",
    "    return \" \".join([str(row[\"title\"]), str(row[\"description\"]), str(row[\"text\"])])\n",
    "\n",
    "content[\"final_text\"] = content.apply(build_text, axis=1)\n",
    "topics[\"text\"] = topics[[\"title\", \"description\"]].agg(\" \".join, axis=1)\n",
    "\n",
    "correlations[\"content_ids\"] = correlations[\"content_ids\"].str.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BvIURgq0Koiv"
   },
   "source": [
    "### Split Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "04wvCsKFOKjy"
   },
   "source": [
    "To evaluate the modelâ€™s generalization ability, I set aside 20% of the content data for final validation using a stratified split by language. This step is fundamental in machine learning to ensure the model performs well on unseen data, not just the training set. I chose to stratify by language to preserve the original language distribution in both the training and validation sets, which is especially important in multilingual text tasks. **Given the limited time**, this was a practical approach to maintain balance. With more time, I would explore more advanced sampling strategiesâ€”such as stratifying by content type or topic coverageâ€”to further improve generalization and robustness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "id": "lE3H8W59KsR_",
    "outputId": "20c569b0-293b-4520-9369-57ad104bfe11"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Split 20% stratified by language\n",
    "content_80, content_20 = train_test_split(\n",
    "    content,\n",
    "    test_size=0.2,\n",
    "    stratify=content['language'],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Get sorted language order by original frequency\n",
    "lang_order = content['language'].value_counts().index\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(20, 5))\n",
    "\n",
    "# Original distribution\n",
    "sns.countplot(x=\"language\", data=content, order=lang_order, ax=axes[0])\n",
    "axes[0].set_title('Original Language Distribution')\n",
    "axes[0].tick_params(axis='x', rotation=90)\n",
    "\n",
    "# 20% Sample distribution\n",
    "sns.countplot(x=\"language\", data=content_20, order=lang_order, ax=axes[1])\n",
    "axes[1].set_title('20% Sample Language Distribution')\n",
    "axes[1].tick_params(axis='x', rotation=90)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CmHdGdANKdPA"
   },
   "source": [
    "### Retrievel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JP8V6wR4P1n1"
   },
   "source": [
    "To efficiently handle the large-scale retrieval taskâ€”approximately 150,000 content pieces and 70,000 topicsâ€”I used the `sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2` model to encode both topics and content into dense embeddings. This multilingual model is compact and optimized for speed, making it ideal given the time constraint and the need to process hundreds of thousands of text pairs quickly. Since our preprocessing step only replaced missing fields with empty strings and preserved all other textual information, using this lightweight model did not compromise representation quality. Had we had more time, heavier but more expressive models like `distiluse-base-multilingual-cased-v1`, `all-mpnet-base-v2`, or `distilbert-base-multilingual-cased` (e.g., DABERT) could have been explored for potentially better semantic coverage. Once embeddings were computed, we used FAISS with L2 distance to retrieve the top-50 most similar topics for each content item. To assess retrieval performance, we computed standard ranking metricsâ€”Precision\\@k, Recall\\@k, F1\\@k, MRR, NDCG, and Coverageâ€”using a custom evaluator. These metrics provide a comprehensive view of how well the retrieval system surfaces relevant topics among its top-ranked candidates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "4DuHT8bR_Cwo"
   },
   "outputs": [],
   "source": [
    "# @title Retrieval Metrics for Evaluate\n",
    "from collections import defaultdict\n",
    "from typing import List, Dict, Union\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class TopicRecommendationEvaluator:\n",
    "    \"\"\"\n",
    "    Complete system for evaluating topic recommendation performance.\n",
    "    Accepts y_true and y_pred as lists of lists or dictionaries.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, csv_path: str = None, df: pd.DataFrame = None):\n",
    "        \"\"\"\n",
    "        Initializes the evaluator with a DataFrame or CSV file.\n",
    "\n",
    "        Args:\n",
    "            csv_path: Optional path to a CSV file.\n",
    "            df: Optional preloaded DataFrame.\n",
    "        \"\"\"\n",
    "        self.df = None\n",
    "        if df is not None:\n",
    "            self.df = self._prepare_data(df)\n",
    "        elif csv_path:\n",
    "            self.df = self._prepare_data(pd.read_csv(csv_path))\n",
    "\n",
    "    def _prepare_data(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Optional pre-processing step (placeholder if needed).\"\"\"\n",
    "        return df\n",
    "\n",
    "    def precision_at_k(self, y_true: List[str], y_pred: List[str], k: int) -> float:\n",
    "        \"\"\"Precision@k: percentage of correct predictions among top-k.\"\"\"\n",
    "        if k == 0 or len(y_pred) == 0:\n",
    "            return 0.0\n",
    "        top_k = y_pred[:k]\n",
    "        relevant = [t for t in top_k if t in y_true]\n",
    "        return len(relevant) / min(k, len(top_k))\n",
    "\n",
    "    def recall_at_k(self, y_true: List[str], y_pred: List[str], k: int) -> float:\n",
    "        \"\"\"Recall@k: percentage of relevant topics retrieved in top-k.\"\"\"\n",
    "        if len(y_true) == 0:\n",
    "            return 0.0\n",
    "        top_k = y_pred[:k]\n",
    "        relevant = [t for t in top_k if t in y_true]\n",
    "        return len(relevant) / len(y_true)\n",
    "\n",
    "    def f1_at_k(self, y_true: List[str], y_pred: List[str], k: int) -> float:\n",
    "        \"\"\"F1@k: harmonic mean between precision and recall.\"\"\"\n",
    "        precision = self.precision_at_k(y_true, y_pred, k)\n",
    "        recall = self.recall_at_k(y_true, y_pred, k)\n",
    "        if precision + recall == 0:\n",
    "            return 0.0\n",
    "        return 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "    def mrr_at_k(self, y_true: List[str], y_pred: List[str], k: int) -> float:\n",
    "        \"\"\"MRR@k: Mean Reciprocal Rank of the first relevant item.\"\"\"\n",
    "        top_k = y_pred[:k]\n",
    "        for i, topic in enumerate(top_k, 1):\n",
    "            if topic in y_true:\n",
    "                return 1.0 / i\n",
    "        return 0.0\n",
    "\n",
    "    def ndcg_at_k(self, y_true: List[str], y_pred: List[str], k: int) -> float:\n",
    "        \"\"\"NDCG@k: Normalized Discounted Cumulative Gain.\"\"\"\n",
    "        def dcg(relevances):\n",
    "            return sum((2**rel - 1) / np.log2(i + 2) for i, rel in enumerate(relevances))\n",
    "\n",
    "        top_k = y_pred[:k]\n",
    "        relevances = [1 if topic in y_true else 0 for topic in top_k]\n",
    "\n",
    "        if sum(relevances) == 0:\n",
    "            return 0.0\n",
    "\n",
    "        dcg_score = dcg(relevances)\n",
    "        ideal_relevances = [1] * min(len(y_true), k)\n",
    "        idcg_score = dcg(ideal_relevances)\n",
    "\n",
    "        return dcg_score / idcg_score if idcg_score > 0 else 0.0\n",
    "\n",
    "    def coverage_at_k(self, y_true: List[str], y_pred: List[str], k: int) -> float:\n",
    "        \"\"\"Coverage@k: at least one relevant topic in top-k.\"\"\"\n",
    "        top_k = y_pred[:k]\n",
    "        return 1.0 if any(topic in y_true for topic in top_k) else 0.0\n",
    "\n",
    "    def hits_at_k(self, y_true: List[str], y_pred: List[str], k: int) -> float:\n",
    "        \"\"\"Hits@k: alias for coverage@k (boolean hit).\"\"\"\n",
    "        return self.coverage_at_k(y_true, y_pred, k)\n",
    "\n",
    "    def evaluate(self,\n",
    "                 y_true: Union[List[List[str]], Dict[str, List[str]]],\n",
    "                 y_pred: Union[List[List[str]], Dict[str, List[str]]],\n",
    "                 k_values: List[int] = [1, 3, 5, 10, 50]) -> Dict[str, Dict[int, float]]:\n",
    "        \"\"\"\n",
    "        Evaluate all metrics at different cutoffs (k).\n",
    "\n",
    "        Args:\n",
    "            y_true: List of true topic lists or dict {content_id: [topics]}.\n",
    "            y_pred: List of predicted topic lists or dict {content_id: [topics]}.\n",
    "            k_values: List of cutoff values to evaluate at.\n",
    "\n",
    "        Returns:\n",
    "            Dictionary of averaged metrics per k.\n",
    "        \"\"\"\n",
    "        # Align data\n",
    "        if isinstance(y_true, dict) and isinstance(y_pred, dict):\n",
    "            common_ids = list(set(y_true.keys()) & set(y_pred.keys()))\n",
    "            true_lists = [y_true[id_] for id_ in common_ids]\n",
    "            pred_lists = [y_pred[id_] for id_ in common_ids]\n",
    "        elif isinstance(y_true, list) and isinstance(y_pred, list):\n",
    "            if len(y_true) != len(y_pred):\n",
    "                raise ValueError(f\"y_true and y_pred must have same length. Got {len(y_true)} vs {len(y_pred)}\")\n",
    "            true_lists = y_true\n",
    "            pred_lists = y_pred\n",
    "        else:\n",
    "            raise ValueError(\"y_true and y_pred must both be lists or both be dicts\")\n",
    "\n",
    "        results = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "        # Compute metrics for each sample\n",
    "        for true_topics, pred_topics in zip(true_lists, pred_lists):\n",
    "            true_topics = list(true_topics)\n",
    "            pred_topics = list(pred_topics)\n",
    "\n",
    "            for k in k_values:\n",
    "                results['Precision@k'][k].append(self.precision_at_k(true_topics, pred_topics, k))\n",
    "                results['Recall@k'][k].append(self.recall_at_k(true_topics, pred_topics, k))\n",
    "                results['F1@k'][k].append(self.f1_at_k(true_topics, pred_topics, k))\n",
    "                results['MRR@k'][k].append(self.mrr_at_k(true_topics, pred_topics, k))\n",
    "                results['NDCG@k'][k].append(self.ndcg_at_k(true_topics, pred_topics, k))\n",
    "                results['Coverage@k'][k].append(self.coverage_at_k(true_topics, pred_topics, k))\n",
    "                results['Hits@k'][k].append(self.hits_at_k(true_topics, pred_topics, k))\n",
    "\n",
    "        # Aggregate metrics\n",
    "        final_results = {}\n",
    "        for metric, k_dict in results.items():\n",
    "            final_results[metric] = {}\n",
    "            for k, values in k_dict.items():\n",
    "                final_results[metric][k] = np.mean(values)\n",
    "\n",
    "        return final_results\n",
    "\n",
    "    def format_results(self, results: Dict[str, Dict[int, float]]) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Format the results as a readable table.\n",
    "\n",
    "        Args:\n",
    "            results: Output dictionary from `evaluate()`.\n",
    "\n",
    "        Returns:\n",
    "            Formatted pandas DataFrame.\n",
    "        \"\"\"\n",
    "        formatted_data = []\n",
    "        k_values = [1, 3, 5, 10, 50]\n",
    "\n",
    "        for metric in ['Precision@k', 'Recall@k', 'F1@k', 'MRR@k', 'NDCG@k', 'Coverage@k', 'Hits@k']:\n",
    "            if metric in results:\n",
    "                row = {'Metric': metric}\n",
    "                for k in k_values:\n",
    "                    row[f'@{k}'] = f\"{results[metric].get(k, 0):.4f}\"\n",
    "                formatted_data.append(row)\n",
    "\n",
    "        return pd.DataFrame(formatted_data)\n",
    "\n",
    "# Utility function for quick evaluation\n",
    "def evaluate_predictions(y_true: List[List[str]],\n",
    "                         y_pred: List[List[str]],\n",
    "                         k_values: List[int] = [1, 3, 5, 10, 50]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Quick wrapper function to evaluate topic predictions.\n",
    "\n",
    "    Args:\n",
    "        y_true: Ground truth topic lists.\n",
    "        y_pred: Predicted topic lists.\n",
    "        k_values: Cutoffs to evaluate at.\n",
    "\n",
    "    Returns:\n",
    "        Formatted evaluation DataFrame.\n",
    "    \"\"\"\n",
    "    evaluator = TopicRecommendationEvaluator()\n",
    "    results = evaluator.evaluate(y_true, y_pred, k_values)\n",
    "    return evaluator.format_results(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5ahi5DvrKSYX"
   },
   "outputs": [],
   "source": [
    "# Desabilita o WandB\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "# ----------------------------------------\n",
    "# âœ… Carrega o modelo e gera embeddings\n",
    "# ----------------------------------------\n",
    "# Removed the 'model_type' argument\n",
    "model = SentenceTransformer(\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "\n",
    "\n",
    "#content_embeddings = model.encode(content_80[\"final_text\"].tolist(), batch_size=128, show_progress_bar=True)\n",
    "#topic_embeddings = model.encode(topics[\"text\"].fillna(\"\").tolist(), batch_size=128, show_progress_bar=True)\n",
    "\n",
    "topic_embeddings_f32 = np.load(\"topic_embeddings_f32.npy\")\n",
    "content_embeddings_f32 = np.load(\"content_embeddings_f32.npy\")\n",
    "\n",
    "# Transforma os embeddings em float32 (requisito do FAISS)\n",
    "topic_embeddings_f32 = np.array(topic_embeddings_f32).astype(\"float32\")\n",
    "content_embeddings_f32 = np.array(content_embeddings_f32).astype(\"float32\")\n",
    "\n",
    "# Cria o Ã­ndice FAISS (Flat = exato, usa L2)\n",
    "index = faiss.IndexFlatL2(topic_embeddings_f32.shape[1])\n",
    "index.add(topic_embeddings_f32)  # Adiciona os tÃ³picos ao Ã­ndice\n",
    "\n",
    "# Consulta os top-k mais prÃ³ximos para cada conteÃºdo\n",
    "k = 50\n",
    "_, indices = index.search(content_embeddings_f32, k)  # [num_contents, k]\n",
    "\n",
    "# Gera os dicionÃ¡rios de prediÃ§Ãµes\n",
    "topic_ids = topics[\"id\"].tolist()\n",
    "content_ids = content_80[\"id\"].tolist()\n",
    "\n",
    "y_pred_dict = {\n",
    "    cid: [topic_ids[i] for i in idx_row]\n",
    "    for cid, idx_row in zip(content_ids, indices)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "id": "nXjlJESnLk6y",
    "outputId": "99001823-ffcc-4ea8-9b55-4427731f2d45"
   },
   "outputs": [],
   "source": [
    "df = correlations.copy(deep=True)\n",
    "df_exploded = df.explode(\"content_ids\")\n",
    "y_true_dict = df_exploded.groupby(\"content_ids\")[\"topic_id\"].apply(list)\n",
    "\n",
    "# Ordena os content_ids que estÃ£o no y_true\n",
    "common_content_ids = sorted(set(y_true_dict.index).intersection(y_pred_dict.keys()))\n",
    "\n",
    "# Cria listas alinhadas de y_true e y_pred\n",
    "y_true = [y_true_dict[cid] for cid in common_content_ids]\n",
    "y_pred = [y_pred_dict[cid] for cid in common_content_ids]\n",
    "\n",
    "\n",
    "true = y_true\n",
    "pred = y_pred\n",
    "\n",
    "evaluate_predictions(true, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7NR1v6NsQT1q"
   },
   "source": [
    "\n",
    "Our retrieval results show a solid baseline performance given the constraints. With `k=50`, we achieved a **Recall\\@50 of 0.2633** and **Coverage\\@50 of 0.3148**, meaning that in roughly 31% of the cases, at least one correct topic appears among the top 50 retrieved. While we currently retrieve the top 50 candidates, increasing the retrieval size to **top 100 or 200** would likely boost recall and model flexibility during re-ranking. Additionally, using a more expressive embedding model like `all-mpnet-base-v2`â€”which captures deeper semantic relationshipsâ€”could further improve retrieval quality, especially for complex or subtle topic-content matches. Considering that top Kaggle solutions for a similar (though not identical) task reach F2 scores around **0.56**, our retrieval coverage of \\~31% suggests that reaching an F2 of around **0.45 after re-ranking** is a realistic and competitive target. This validates our decision to proceed with `k=50` as a solid tradeoff between speed and candidate quality.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xzqu88KfS7V4"
   },
   "source": [
    "Create our dataset for Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4d3LHrRNQLh2"
   },
   "outputs": [],
   "source": [
    "# ----------------------------------------\n",
    "# âœ… Mapeia posiÃ§Ãµes para IDs\n",
    "# ----------------------------------------\n",
    "topic_ids = topics[\"id\"].tolist()\n",
    "content_ids = content_80[\"id\"].tolist()\n",
    "\n",
    "# FAISS indices â†’ topic_ids\n",
    "index_to_topic = {i: tid for i, tid in enumerate(topic_ids)}\n",
    "content_id_array = np.array(content_ids)\n",
    "\n",
    "# ----------------------------------------\n",
    "# âœ… Prepara lookup para pares positivos\n",
    "# ----------------------------------------\n",
    "correlations_exploded = correlations.explode(\"content_ids\").dropna()\n",
    "correlation_set = set(zip(correlations_exploded[\"content_ids\"], correlations_exploded[\"topic_id\"]))\n",
    "\n",
    "# ----------------------------------------\n",
    "# âœ… Cria lookup de textos\n",
    "# ----------------------------------------\n",
    "content_text_dict = dict(zip(content_80[\"id\"], content_80[\"final_text\"]))\n",
    "topic_text_dict = dict(zip(topics[\"id\"], topics[\"text\"]))\n",
    "\n",
    "# ----------------------------------------\n",
    "# âœ… Cria pares FAISS (content_id, topic_id, label, t, text)\n",
    "# ----------------------------------------\n",
    "data = []\n",
    "for content_idx, topic_ranked_indices in enumerate(indices):\n",
    "    cid = content_id_array[content_idx]\n",
    "    for tidx in topic_ranked_indices:\n",
    "        tid = index_to_topic[tidx]\n",
    "        label = 1 if (cid, tid) in correlation_set else 0\n",
    "        content_text = content_text_dict.get(cid, \"\")\n",
    "        topic_text = topic_text_dict.get(tid, \"\")\n",
    "        data.append((cid, tid, label, content_text, topic_text))\n",
    "\n",
    "df_pairs = pd.DataFrame(data, columns=[\"content_id\", \"topic_id\", \"label\", \"final_text\", \"text\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OzEB8SB1M508"
   },
   "source": [
    "###  Ranking(Classification) Content to Topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zCp03ZNITM5d"
   },
   "source": [
    "#### Feature Engineer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E7-CzUwaX2KO"
   },
   "source": [
    "For the feature engineering process, I focused on approaches that could be executed efficiently within the 3-hour constraint while still capturing meaningful signals between content and topics. Since we had already applied global semantic encoding to retrievel, I complemented this with **local semantic signals** by training a **Word2Vec** model on the available texts. This model is fast to train, works well for short-form educational language, for each contentâ€“topic pair.\n",
    "\n",
    "To further enrich the feature space, I computed **TF-IDF-weighted overlaps** to capture how frequently important educational terms intersect across descriptions and titles. Our EDA revealed consistent and recurring keywordsâ€”like *equations*, *functions*, *module*, *student*, and *math*â€”indicating strong thematic alignment between content and topic metadata.\n",
    "\n",
    "\n",
    "Hence, I combined it with **Word2vec** to capture more fine-grained lexical signals. Specifically, for each word in the text, we compute a weighted embedding as:\n",
    "\n",
    "$$\n",
    "\\vec{v}_{\\text{text}} = \\frac{1}{N} \\sum_{i=1}^{N} \\text{TFIDF}(w_i) \\cdot \\vec{v}_{w_i}\n",
    "$$\n",
    "\n",
    "where $\\vec{v}_{w_i}$ is the Word2Vec embedding of word $w_i$, and $\\text{TFIDF}(w_i)$ is its TF-IDF weight. This method is described in **Chapter 6** of *Text Analytics with Python (2nd Edition)* by **Dipanjan Sarkar**, where it is used for both **retrieval and classification** tasks, this strategy provides a rich and efficient feature set under our time constraint.\n",
    "\n",
    "\n",
    "\n",
    "Finally, I included **basic statistical features** (e.g., length differences, token counts, cosine distance between mean vectors) to capture structural patterns. Together, these handcrafted features aim to enhance the ranking model by combining semantic, lexical, and structural cues.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s3EvS5rkaWBU"
   },
   "outputs": [],
   "source": [
    "# Tokenize the text in each row\n",
    "tokenized_texts = topics[\"text\"].apply(lambda x: word_tokenize(x.lower()))\n",
    "\n",
    "# Convert to a list of lists for Word2Vec training\n",
    "sentences = tokenized_texts.tolist()\n",
    "\n",
    "# ----------------------------------------\n",
    "# âœ… Train the Word2Vec model\n",
    "# ----------------------------------------\n",
    "model_wv = Word2Vec(\n",
    "    sentences=sentences,\n",
    "    vector_size=32,     # Size of the embedding vector\n",
    "    window=2,           # Context window size\n",
    "    min_count=1,        # Minimum frequency to include the word\n",
    "    workers=4,          # Number of threads\n",
    "    sg=0                # Use skip-gram (1) instead of CBOW (0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "780ce0e1928342bb9ac257190c22c20d",
      "951d8ec82e4a4abba485d01e95bbdb54",
      "a81a81b8d6ea4aee9198dc82887e74c0",
      "92fb396e02dc4f71b3ad9702a9ef61f5",
      "eb65d300c9de4fe5b3546618c126521c",
      "991fb3b5a66a4cc29895ebfb536bee2f",
      "7c2e58df588c42df8776386d5bc6225d",
      "5ef6b96a510b4aa2855ccd059fccd5f2",
      "a20847fd53594fd5b307c93e0049e0b7",
      "e8fb5dafe83d4809b2be3c14f58c3eec",
      "e60c4b70c72a4680bca363c9b8daa5a4",
      "4cdaa5c624da4fc9b0f818cd938f3132",
      "e49e6515ee05464098da183fa306f267",
      "614bb95a2ad647bb9dacf0784e71b586",
      "8f1e385954e842d9bce4aa128f6eb9e3",
      "34741af0b8b041fcbc8337e108dad143",
      "7d62e8d5399e4ee69f8c2a2d2d70899c",
      "7e73c5af7a8a488291f2dcb7292aff05",
      "e9356c573fc04147a58f57f67f31faff",
      "c5d1f94812c0413e8f93bbde2b8d6f27",
      "5966fb48f77944ff8b9ebb578cdbe25f",
      "0bafe861b8144df98af7f7f9943915b2"
     ]
    },
    "id": "Gmf-lZREhmp6",
    "outputId": "e2c9f967-5d38-4fc2-dc28-2925341e9fd8"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "# --------------------------------------------------\n",
    "# âœ… Load and merge stopwords from all languages in the dataset\n",
    "# --------------------------------------------------\n",
    "languages = topics['language'].unique().tolist()\n",
    "\n",
    "combined_stops = set()\n",
    "for lang in languages:\n",
    "    try:\n",
    "        combined_stops.update(stopwords.stopwords(lang))\n",
    "    except:\n",
    "        print(f\"Could not load stop words for language code: {lang}. Skipping.\")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# âœ… Fit TF-IDF on the topic texts\n",
    "# --------------------------------------------------\n",
    "tfidf = TfidfVectorizer(max_features=18953, stop_words=list(combined_stops))\n",
    "tfidf_matrix = tfidf.fit_transform(topics[\"text\"])\n",
    "tfidf_vocab = tfidf.vocabulary_  # {word: index}\n",
    "idf_values = tfidf.idf_          # numpy array\n",
    "\n",
    "# Create dict: {word: idf_value}\n",
    "idf_dict = {word: idf_values[idx] for word, idx in tfidf_vocab.items()}\n",
    "\n",
    "# --------------------------------------------------\n",
    "# âœ… Encode text by weighted sum of word2vec[word] * tf-idf[word]\n",
    "# --------------------------------------------------\n",
    "def encode_text_with_tfidf_word2vec(text, model_wv, idf_dict):\n",
    "    tokens = simple_preprocess(text, deacc=True)\n",
    "    vecs = []\n",
    "\n",
    "    for word in tokens:\n",
    "        if word in model_wv.wv:\n",
    "            tfidf_weight = idf_dict.get(word, 1.0)\n",
    "            vecs.append(tfidf_weight * model_wv.wv[word])\n",
    "\n",
    "    if vecs:\n",
    "        return np.mean(vecs, axis=0)  # Or np.sum(vecs, axis=0) if preferred\n",
    "    else:\n",
    "        return np.zeros(model_wv.vector_size)\n",
    "\n",
    "# --------------------------------------------------\n",
    "# âœ… Apply to both content and topics\n",
    "# --------------------------------------------------\n",
    "content_80[\"word2vec_embedding\"] = content_80[\"final_text\"].swifter.apply(lambda x: encode_text_with_tfidf_word2vec(x, model_wv, idf_dict))\n",
    "topics[\"word2vec_embedding\"] = topics[\"text\"].swifter.apply(lambda x: encode_text_with_tfidf_word2vec(x, model_wv, idf_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GQcR6ZXtp9hu",
    "outputId": "5002f0e9-6238-434e-af93-e5622d91a24a"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "# ----------------------------------------\n",
    "# âœ… Embedding dictionaries\n",
    "# ----------------------------------------\n",
    "content_embed_dict = dict(zip(content[\"id\"], content_80[\"word2vec_embedding\"]))\n",
    "topic_embed_dict = dict(zip(topics[\"id\"], topics[\"word2vec_embedding\"]))\n",
    "\n",
    "# ----------------------------------------\n",
    "# âœ… Extract raw vectors using tqdm and .map()\n",
    "# ----------------------------------------\n",
    "df_pairs[\"content_vec\"] = df_pairs[\"content_id\"].progress_map(content_embed_dict.get)\n",
    "df_pairs[\"topic_vec\"] = df_pairs[\"topic_id\"].progress_map(topic_embed_dict.get)\n",
    "\n",
    "# Remove pairs with any missing vector (None)\n",
    "df_pairs = df_pairs.dropna(subset=[\"content_vec\", \"topic_vec\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-Bo8-j7M7w0W",
    "outputId": "46e44850-e6f3-47ab-cbeb-e7631efe786e"
   },
   "outputs": [],
   "source": [
    "# ----------------------------------------\n",
    "# âœ… Convert embedding lists into NumPy arrays\n",
    "# ----------------------------------------\n",
    "content_vecs = np.vstack(df_pairs[\"content_vec\"].values)\n",
    "topic_vecs = np.vstack(df_pairs[\"topic_vec\"].values)\n",
    "\n",
    "# ----------------------------------------\n",
    "# âœ… Compute vector-based features: similarities and distances\n",
    "# ----------------------------------------\n",
    "dot_products = np.sum(content_vecs * topic_vecs, axis=1)\n",
    "norms_content = norm(content_vecs, axis=1)\n",
    "norms_topic = norm(topic_vecs, axis=1)\n",
    "cosine_sims = dot_products / (norms_content * norms_topic + 1e-9)\n",
    "\n",
    "euclidean_dists = norm(content_vecs - topic_vecs, axis=1)\n",
    "abs_diff_means = np.mean(np.abs(content_vecs - topic_vecs), axis=1)\n",
    "\n",
    "# ----------------------------------------\n",
    "# âœ… Compute word overlap and lengths (with vectorized map)\n",
    "# ----------------------------------------\n",
    "def count_overlap_and_lengths(t, text):\n",
    "    content_words = set(str(t).lower().split())\n",
    "    topic_words = set(str(text).lower().split())\n",
    "    return (\n",
    "        len(content_words & topic_words),\n",
    "        len(content_words),\n",
    "        len(topic_words)\n",
    "    )\n",
    "\n",
    "# Wrap zip(...) with tqdm to show progress\n",
    "word_stats = np.array([\n",
    "    count_overlap_and_lengths(t, text)\n",
    "    for t, text in tqdm(zip(df_pairs[\"final_text\"], df_pairs[\"text\"]), total=len(df_pairs), desc=\"Calculating overlap features\")\n",
    "])\n",
    "\n",
    "# ----------------------------------------\n",
    "# âœ… Add all features to the DataFrame\n",
    "# ----------------------------------------\n",
    "df_pairs[\"cosine_sim\"] = cosine_sims\n",
    "df_pairs[\"euclidean_dist\"] = euclidean_dists\n",
    "df_pairs[\"abs_diff_mean\"] = abs_diff_means\n",
    "df_pairs[\"dot_product\"] = dot_products\n",
    "\n",
    "df_pairs[\"word_intersection\"] = word_stats[:, 0]\n",
    "df_pairs[\"len_content_words\"] = word_stats[:, 1]\n",
    "df_pairs[\"len_topic_words\"] = word_stats[:, 2]\n",
    "\n",
    "# ----------------------------------------\n",
    "# âœ… Prepare final input matrix X and target vector y\n",
    "# ----------------------------------------\n",
    "X_embed = np.hstack([content_vecs, topic_vecs])\n",
    "X_extra = df_pairs[\n",
    "    [\n",
    "        \"cosine_sim\", \"euclidean_dist\", \"abs_diff_mean\", \"dot_product\",\n",
    "        \"word_intersection\", \"len_content_words\", \"len_topic_words\"\n",
    "    ]\n",
    "].values\n",
    "\n",
    "X = np.hstack([X_embed, X_extra])\n",
    "y = df_pairs[\"label\"].values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5JwE6BtAVnFM"
   },
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "282WorvbgBcp"
   },
   "source": [
    "To optimize the LightGBM re-ranking model, I performed a 5-fold stratified cross-validation over combinations of n_estimators and SMOTE sampling strategies. This setup allows evaluation across balanced folds while addressing class imbalance in the training data. I tested n_estimators values of 100, 500, and 1000, and SMOTE ratios of 0.3 and 0.7. For each fold, I computed Precision, Recall, F1, and F2 scores, F2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rbe8V3hpVvGe"
   },
   "outputs": [],
   "source": [
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for z in [1000,500]:\n",
    "    for x in [0.3]:\n",
    "        precisions = []\n",
    "        recalls = []\n",
    "        f1s = []\n",
    "        f2s = []\n",
    "\n",
    "        for train_idx, val_idx in kfold.split(X, y):\n",
    "            X_train, X_val = X[train_idx], X[val_idx]\n",
    "            y_train, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "            smote = SMOTE(sampling_strategy=x, random_state=42)\n",
    "            X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "            model_light = LGBMClassifier(n_estimators=z, random_state=42,verbose=0)\n",
    "            model_light.fit(X_train_res, y_train_res)\n",
    "\n",
    "            y_pred = model_light.predict(X_val)\n",
    "\n",
    "            precisions.append(precision_score(y_val, y_pred, average='binary'))\n",
    "            recalls.append(recall_score(y_val, y_pred, average='binary'))\n",
    "            f1s.append(f1_score(y_val, y_pred, average='binary'))\n",
    "            f2s.append(fbeta_score(y_val, y_pred, beta=2, average='binary'))\n",
    "\n",
    "        print(f\"\\n=== Params: n_estimators={z}, sampling_strategy={x} ===\")\n",
    "        print(f\"Avg Precision: {np.mean(precisions):.4f}\")\n",
    "        print(f\"Avg Recall:    {np.mean(recalls):.4f}\")\n",
    "        print(f\"Avg F1-score:  {np.mean(f1s):.4f}\")\n",
    "        print(f\"Avg F2-score:  {np.mean(f2s):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "esov79jkcoyH"
   },
   "source": [
    "### Test Predict (Prodcution Model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3tUWyZL_hDqy"
   },
   "source": [
    "This final evaluation step validates the effectiveness of the full retrieval and ranking pipeline. Using the held-out content_20.csv dataset, the model achieved strong generalization, with high Recall and a competitive F2 score, confirming its ability to recover relevant topicâ€“content pairs. The ROC AUC further supports the modelâ€™s discriminative power. These results demonstrate that our retrieval + re-ranking approach is not only efficient but also reliable for real-world application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L8IBQg3Bczil",
    "outputId": "64b8acbd-813b-437d-9f03-6ac1573a037b"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "import faiss\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# === Ensure folders exist ===\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "# âœ… Save trained LightGBM model\n",
    "joblib.dump(model_light, \"models/lgbm_model.pkl\")\n",
    "\n",
    "# âœ… Save TF-IDF vectorizer and IDF dictionary\n",
    "joblib.dump(tfidf, \"models/tfidf.pkl\")\n",
    "joblib.dump(idf_dict, \"models/idf_dict.pkl\")\n",
    "\n",
    "# âœ… Save Word2Vec model\n",
    "model_wv.save(\"models/word2vec_topics.model\")\n",
    "\n",
    "# âœ… Save SentenceTransformer model\n",
    "model.save(\"sentence_model\")\n",
    "\n",
    "# âœ… Save FAISS index for retrieval\n",
    "faiss.write_index(index, \"models/faiss_index.index\")\n",
    "\n",
    "# âœ… Save metadata files\n",
    "topics.to_csv(\"data/topics.csv\", index=False)\n",
    "content_20.to_csv(\"data/content_20.csv\", index=False)\n",
    "correlations_exploded.to_csv(\"data/correlations_exploded.csv\", index=False)\n",
    "\n",
    "# âœ… Save SentenceTransformer embeddings (retrieval stage)\n",
    "np.save(\"data/topic_embeddings_f32.npy\", topic_embeddings_f32)\n",
    "\n",
    "# âœ… Save precomputed Word2Vec+TFIDF embeddings\n",
    "topic_embed_matrix = np.vstack(topics[\"word2vec_embedding\"].values)\n",
    "np.save(\"data/topic_word2vec_tfidf_embeddings.npy\", topic_embed_matrix)\n",
    "print(\"âœ… All model artifacts, metadata, and embeddings saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "07d98976",
    "outputId": "90ea871d-e556-4947-ec81-83693f818576"
   },
   "outputs": [],
   "source": [
    "from numpy.linalg import norm\n",
    "# ----------------------------------------\n",
    "# âœ… Load saved artifacts\n",
    "# ----------------------------------------\n",
    "model = joblib.load(\"models/lgbm_model.pkl\")\n",
    "tfidf = joblib.load(\"models/tfidf.pkl\")\n",
    "idf_dict = joblib.load(\"models/idf_dict.pkl\")\n",
    "w2v = Word2Vec.load(\"models/word2vec_topics.model\")\n",
    "sentence_model = SentenceTransformer(\"sentence_model\")\n",
    "index = faiss.read_index(\"models/faiss_index.index\")\n",
    "\n",
    "topics = pd.read_csv(\"data/topics.csv\")\n",
    "content_20 = pd.read_csv(\"data/content_20.csv\")\n",
    "correlations = pd.read_csv(\"data/correlations_exploded.csv\")\n",
    "\n",
    "# ----------------------------------------\n",
    "# âœ… Load precomputed embeddings\n",
    "# ----------------------------------------\n",
    "topic_word2vec_embed = np.load(\"data/topic_word2vec_tfidf_embeddings.npy\")\n",
    "\n",
    "# Create topic dictionaries\n",
    "topic_id_list = topics[\"id\"].tolist()\n",
    "topic_embed_dict = dict(zip(topic_id_list, topic_word2vec_embed))\n",
    "topic_text_dict = dict(zip(topic_id_list, topics[\"text\"]))\n",
    "\n",
    "# ----------------------------------------\n",
    "# âœ… Build content_20 embeddings\n",
    "# ----------------------------------------\n",
    "def encode_text_with_tfidf_word2vec(text, model, idf_dict):\n",
    "    tokens = simple_preprocess(str(text), deacc=True)\n",
    "    vecs = []\n",
    "    for word in tokens:\n",
    "        if word in model.wv:\n",
    "            tfidf_weight = idf_dict.get(word, 1.0)\n",
    "            vecs.append(tfidf_weight * model.wv[word])\n",
    "    if vecs:\n",
    "        return np.mean(vecs, axis=0)\n",
    "    else:\n",
    "        return np.zeros(model.vector_size)\n",
    "\n",
    "content_20[\"word2vec_embedding\"] = content_20[\"final_text\"].apply(lambda x: encode_text_with_tfidf_word2vec(x, w2v, idf_dict))\n",
    "content_vecs = np.vstack(content_20[\"word2vec_embedding\"].values)\n",
    "\n",
    "# Encode SentenceTransformer vectors and normalize\n",
    "content_vecs_st = sentence_model.encode(content_20[\"final_text\"].tolist(), batch_size=128, show_progress_bar=True)\n",
    "content_vecs_st = np.array(content_vecs_st).astype(\"float32\")\n",
    "\n",
    "# ----------------------------------------\n",
    "# âœ… Retrieve top-K topics using FAISS\n",
    "# ----------------------------------------\n",
    "k = 50\n",
    "_, retrieved_indices = index.search(content_vecs_st, k)\n",
    "\n",
    "# ----------------------------------------\n",
    "# âœ… Prepare prediction pairs\n",
    "# ----------------------------------------\n",
    "data = []\n",
    "content_ids = content_20[\"id\"].tolist()\n",
    "content_texts = content_20[\"final_text\"].tolist()\n",
    "content_embed_dict = dict(zip(content_ids, content_vecs))\n",
    "\n",
    "# ----------------------------------------\n",
    "# âœ… Prepare all topic vectors in advance\n",
    "# ----------------------------------------\n",
    "retrieved_topic_indices = retrieved_indices\n",
    "retrieved_topic_ids = [[topic_id_list[idx] for idx in row] for row in retrieved_topic_indices]\n",
    "topic_vectors_batch = [[topic_embed_dict[tid] for tid in row] for row in retrieved_topic_ids]\n",
    "topic_texts_batch = [[topic_text_dict[tid] for tid in row] for row in retrieved_topic_ids]\n",
    "\n",
    "# ----------------------------------------\n",
    "# âœ… Vectorized preparation loop\n",
    "# ----------------------------------------\n",
    "data = []\n",
    "\n",
    "for i, c_id in enumerate(content_ids):\n",
    "    c_vec = content_embed_dict[c_id]\n",
    "    c_text = content_texts[i]\n",
    "    topic_vecs = topic_vectors_batch[i]\n",
    "    topic_ids = retrieved_topic_ids[i]\n",
    "    topic_texts = topic_texts_batch[i]\n",
    "\n",
    "    for t_id, t_vec, t_text in zip(topic_ids, topic_vecs, topic_texts):\n",
    "        # Label if pair is in correlation set\n",
    "        label = int((c_id, t_id) in correlation_set)\n",
    "\n",
    "        # Compute all features\n",
    "        dot = np.dot(c_vec, t_vec)\n",
    "        cosine = dot / (norm(c_vec) * norm(t_vec) + 1e-9)\n",
    "        euclidean = norm(c_vec - t_vec)\n",
    "        abs_diff_mean = np.mean(np.abs(c_vec - t_vec))\n",
    "\n",
    "        content_words = set(c_text.lower().split())\n",
    "        topic_words = set(t_text.lower().split())\n",
    "        intersection = len(content_words & topic_words)\n",
    "        len_c = len(content_words)\n",
    "        len_t = len(topic_words)\n",
    "\n",
    "        # Combine all into one row\n",
    "        features = np.hstack([\n",
    "            c_vec, t_vec,\n",
    "            [cosine, euclidean, abs_diff_mean, dot, intersection, len_c, len_t]\n",
    "        ])\n",
    "        data.append((features, label))\n",
    "\n",
    "\n",
    "# ----------------------------------------\n",
    "# âœ… Predict with LightGBM model\n",
    "# ----------------------------------------\n",
    "X_test = np.array([d[0] for d in data])\n",
    "y_true_all = np.array([d[1] for d in data])\n",
    "y_proba_all = model.predict_proba(X_test)[:, 1]\n",
    "y_pred_all = (y_proba_all > 0.5).astype(int)\n",
    "\n",
    "# ----------------------------------------\n",
    "# âœ… Print final metrics\n",
    "# ----------------------------------------\n",
    "print(\"\\nðŸ“Š Evaluation on content_20.csv:\")\n",
    "print(f\"Precision:  {precision_score(y_true_all, y_pred_all, average='binary'):.4f}\")\n",
    "print(f\"Recall:     {recall_score(y_true_all, y_pred_all, average='binary'):.4f}\")\n",
    "print(f\"F1 Score:   {f1_score(y_true_all, y_pred_all, average='binary'):.4f}\")\n",
    "print(f\"F2 Score:   {fbeta_score(y_true_all, y_pred_all, average='binary', beta=2):.4f}\")\n",
    "print(f\"ROC AUC:    {roc_auc_score(y_true_all, y_proba_all):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9EzvrEr2pb-E"
   },
   "source": [
    "\n",
    "### âœ… Conclusion\n",
    "\n",
    "In this notebook, we developed a full retrieval and ranking pipeline for the topic recommendation task under a strict 3-hour constraint. Our approach combined:\n",
    "\n",
    "* **Dense multilingual embeddings** (`paraphrase-multilingual-MiniLM-L12-v2`) for fast semantic retrieval using FAISS;\n",
    "* **Feature engineering** using TF-IDFâ€“weighted Word2Vec embeddings and overlap statistics;\n",
    "* **A LightGBM classifier** trained on contentâ€“topic pairs with SMOTE to mitigate class imbalance.\n",
    "\n",
    "Despite the limited development time and use of lightweight models, the system achieved promising performance:\n",
    "\n",
    "* **Retrieval Recall\\@50**: 0.2633\n",
    "* **Retrieval Coverage\\@50**: 0.3148\n",
    "* **Final model F2 Score**: \\~0.41\n",
    "* **ROC AUC**: 0.96+\n",
    "\n",
    "These results demonstrate that even a streamlined pipeline can yield competitive results on large-scale multilingual data.\n",
    "\n",
    "With more time and resources, future improvements could include:\n",
    "\n",
    "* Replacing the encoder with **larger sentence models** (e.g., `all-mpnet-base-v2`, `mdeberta-v3-base`);\n",
    "* Fine-tuning the embedding model using **contrastive learning** (e.g., SimCSE or TSDAE);\n",
    "* Exploring **LLM-based rerankers** (e.g., GPT with retrieval-augmented generation for scoring pairs);\n",
    "* Using **more sophisticated negative sampling** and better calibration for thresholds.\n",
    "\n",
    "Overall, the solution offers a **fast, modular, and extensible baseline** that can be easily scaled or enhanced for production.\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
